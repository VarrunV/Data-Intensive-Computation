{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MX Youtube Trending Videos Analysis\n",
    "This notebook will load the Mexican\n",
    "Youtube Trending Videos Dataset from `data` folder, and then compute and save to file in the `results` folder the following variables:\n",
    "- Number of videos, channels and categories\n",
    "- Mean and standard deviation of the number of views, comments, likes and dislikes\n",
    "- Most popular categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import useful libraries and functions, then we load data from `data` folder in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country = MX\n",
       "numCountries = 2\n",
       "df = [video_id: string, trending_date: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[video_id: string, trending_date: string ... 14 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.matching\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val country = \"MX\"\n",
    "val numCountries = 2\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/\" + country + \"videos_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first two lines of the DataFrame (including header and the first data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|        publish_time|                tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|SbOwzAl9ZfQ|     17.14.11|Capítulo 12 | Mas...|     MasterChef 2017|         24|2017-11-13T06:06:...|\"MasterChef Junio...|310130| 4182|     361|         1836|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|Disfruta la prese...|\n",
      "|klOV6Xh-DnI|     17.14.11|ALEXA EX-INTEGRAN...|Micky Contreras M...|         22|2017-11-13T05:11:...|     La Voz Mexico 7|104972|  271|     174|          369|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|ALEXA EX-INTEGRAN...|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the number of channels, videos and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nChannels = 6978\n",
       "nVideos = 43819\n",
       "nCategories = 17\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nChannels = df.select(\"channel_title\").distinct.count\n",
    "val nVideos = df.count\n",
    "val nCategories = df.select(\"category_id\").distinct.count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the mean and standard deviation of the number of views, comments, likes and dislikes.\n",
    "\n",
    "We use the function `df.describe()` then transform the results from `double` to `integer` type, in order to remove the decimals.\n",
    "Finally we filter out the values of max, min and number of videos, which are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-----+--------+\n",
      "|summary|  views|comment_count|likes|dislikes|\n",
      "+-------+-------+-------------+-----+--------+\n",
      "|   mean| 342381|         2039|15861|     747|\n",
      "| stddev|1714690|        13938|81089|   10953|\n",
      "+-------+-------+-------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numDataDouble = [summary: string, views: string ... 3 more fields]\n",
       "numData = [summary: string, views: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, views: int ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute mean, stddev, max and min values of likes, dislikes, views and comment_count\n",
    "val numDataDouble = df.describe(\"views\", \"comment_count\", \"likes\", \"dislikes\")\n",
    "\n",
    "val numData = numDataDouble.withColumn(\"views\", col(\"views\").cast(IntegerType))\n",
    "                           .withColumn(\"likes\", col(\"likes\").cast(IntegerType))\n",
    "                           .withColumn(\"dislikes\", col(\"dislikes\").cast(IntegerType))\n",
    "                           .withColumn(\"comment_count\", col(\"comment_count\").cast(IntegerType))\n",
    "                           .filter($\"summary\" =!= \"max\")\n",
    "                           .filter($\"summary\" =!= \"min\")\n",
    "                           .filter($\"summary\" =!= \"count\")\n",
    "//val numDataLong = numData.select(numData.columns.map(c => col(c).cast(IntegerType)) : _*)\n",
    "numData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular channels by grouping the dataset according to channels, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|       channel_title|num_videos|\n",
      "+--------------------+----------+\n",
      "|                null|      3368|\n",
      "|              Cracks|       186|\n",
      "|             Badabun|       184|\n",
      "|      Troom Troom Es|       168|\n",
      "|           Cracks MX|       162|\n",
      "|       Las Estrellas|       155|\n",
      "|Televisa Telenovelas|       149|\n",
      "|      El Salvador 4K|       145|\n",
      "|       Tu COSMOPOLIS|       143|\n",
      "|       Enamorándonos|       136|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByChannel = RelationalGroupedDataset: [grouping expressions: [channel_title: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "channelsVideoCount = [channel_title: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[channel_title: string, num_videos: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByChannel = df.groupBy(\"channel_title\")\n",
    "val channelsVideoCount = groupedByChannel.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "channelsVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular categories by grouping the dataset according to categories, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|category_id|num_videos|\n",
      "+-----------+----------+\n",
      "|         24|     13487|\n",
      "|         22|      8159|\n",
      "|         17|      4050|\n",
      "|         10|      3371|\n",
      "|       null|      3368|\n",
      "|         25|      3113|\n",
      "|         26|      2467|\n",
      "|         23|      1742|\n",
      "|          1|      1298|\n",
      "|         20|       994|\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByCategory = RelationalGroupedDataset: [grouping expressions: [category_id: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "categoriesVideoCount = [category_id: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[category_id: string, num_videos: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByCategory = df.groupBy(\"category_id\")\n",
    "val categoriesVideoCount = groupedByCategory.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "categoriesVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data contained in the `.json` file, with the category ids, matched with the corresponding category names.\n",
    "\n",
    "The `.json` file has a complex structure, which makes it necessary to perform many steps to polish the data and obtain what we need as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|            name|\n",
      "+---+----------------+\n",
      "|  1|Film & Animation|\n",
      "|  2|Autos & Vehicles|\n",
      "| 10|           Music|\n",
      "| 15|  Pets & Animals|\n",
      "| 17|          Sports|\n",
      "+---+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoriesStruct = [etag: string, items: array<struct<etag:string,id:string,kind:string,snippet:struct<assignable:boolean,channelId:string,title:string>>> ... 1 more field]\n",
       "categoriesArray = Array(WrappedArray([\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(WrappedArray([\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load categories names file data\n",
    "val categoriesStruct = spark.read.option(\"multiline\",\"true\")\n",
    "                .json(\"data/\" + country + \"_category_id.json\")\n",
    "// extract the array inside the struct and convert to string\n",
    "val categoriesArray = categoriesStruct.collect()(0)(1)\n",
    "                .toString.split(\",\")\n",
    "// extract categories names and ids\n",
    "val names = categoriesArray.filter(x => x.contains(\"]]\")).map(x => x.dropRight(2))\n",
    "val ids = categoriesArray.filter(x => x.length() < 3)\n",
    "// create a DataFrame out of category ids and names\n",
    "val categoriesNames = sc.parallelize(ids zip names).toDF(\"id\", \"name\")\n",
    "\n",
    "categoriesNames.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join the two DataFrames of top categories and categories names, obtaining a DataFrame containing only categories names and number of videos belonging to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// join the dataframes so the categories are matched with their names\n",
    "val topCategories = categoriesVideoCount.join(categoriesNames, categoriesVideoCount(\"category_id\") === categoriesNames(\"id\"), \"leftouter\")\n",
    "                    .orderBy(desc(\"num_videos\"))\n",
    "                    .select(\"name\", \"num_videos\")\n",
    "                    .filter($\"name\" =!= \"null\")\n",
    "\n",
    "// hide the numeric column in the output.\n",
    "topCategories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed all the data we need, we can create a single-line DataFrame containing the data for this country.\n",
    "\n",
    "We need to transform DataFrames into Lists, which is a structure that enable us to extract single values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numDataList = numData.rdd.collect().toList\n",
    "val topCategoriesList = topCategories.rdd.collect().toList\n",
    "\n",
    "val statsSeq = Seq((country, nChannels.toString, \n",
    "        nVideos.toString, nCategories.toString,\n",
    "        numDataList(0)(1).toString, numDataList(1)(1).toString,\n",
    "        numDataList(0)(2).toString, numDataList(1)(2).toString,\n",
    "        numDataList(0)(3).toString, numDataList(1)(3).toString,\n",
    "        numDataList(0)(4).toString, numDataList(1)(4).toString,\n",
    "        topCategoriesList(1)(0).toString, topCategoriesList(2)(0).toString,\n",
    "        topCategoriesList(3)(0).toString, topCategoriesList(4)(0).toString,\n",
    "        topCategoriesList(5)(0).toString, topCategoriesList(6)(0).toString,\n",
    "        topCategoriesList(7)(0).toString, topCategoriesList(8)(0).toString,\n",
    "        topCategoriesList(9)(0).toString, topCategoriesList(10)(0).toString))\n",
    "\n",
    "val statsRDD = spark.sparkContext.parallelize(statsSeq)\n",
    "val statsDF = statsRDD.toDF(\"country\", \"num_channels\", \n",
    "                            \"num_videos\", \"num_categories\",\n",
    "                            \"views_mean\", \"views_stddev\",\n",
    "                            \"comments_mean\", \"comments_stddev\",\n",
    "                            \"likes_mean\", \"likes_stddev\",\n",
    "                            \"dislikes_mean\", \"dislikes_stddev\",\n",
    "                            \"1_category\", \"2_category\", \n",
    "                            \"3_category\", \"4_category\",\n",
    "                            \"5_category\", \"6_category\", \n",
    "                            \"7_category\", \"8_category\",\n",
    "                            \"9_category\", \"10_category\")\n",
    "statsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load the data computed previously about other countries, and join it with the data from the current country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val allCountriesData = spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"results/partial_\" + numCountries.toString)\n",
    "val newAllCountriesData = allCountriesData.union(statsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the final DataFrame to file, in the `results` folder.\n",
    "\n",
    "We use the function `repartition(1)` in order to generate only one file, instead of separating the data into several different files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newAllCountriesData.repartition(1)\n",
    "             .write\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\")\n",
    "             .save(\"results/partial_\" + (numCountries + 1).toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
