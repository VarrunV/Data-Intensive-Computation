{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JP Youtube Trending Videos Analysis\n",
    "This notebook will load the Japanese Youtube Trending Videos Dataset from `data` folder, and then compute and save to file in the `results` folder the following variables:\n",
    "- Number of videos, channels and categories\n",
    "- Mean and standard deviation of the number of views, comments, likes and dislikes\n",
    "- Most popular categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import useful libraries and functions, then we load data from `data` folder in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country = JP\n",
       "numCountries = 4\n",
       "df = [video_id: string, trending_date: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[video_id: string, trending_date: string ... 14 more fields]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.matching\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val country = \"JP\"\n",
    "val numCountries = 4\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/\" + country + \"videos_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first two lines of the DataFrame (including header and the first data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------------------------------+-------------------------+-----------+--------------------+---------------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+-------------------------------------+\n",
      "|   video_id|trending_date|                               title|            channel_title|category_id|        publish_time|                       tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|                          description|\n",
      "+-----------+-------------+------------------------------------+-------------------------+-----------+--------------------+---------------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+-------------------------------------+\n",
      "|5ugKfHgsmYw|     18.07.02|陸自ヘリ、垂直に落下＝路上の車が撮影|     時事通信映像センター|         25|2018-02-06T03:04:...|\"事故|\"\"佐賀\"\"|\"\"佐賀県\"...|188085|  591|     189|            0|https://i.ytimg.c...|             TRUE|           FALSE|                 FALSE|佐賀県神埼市の民家に墜落した陸上自...|\n",
      "|ohObafdd34Y|     18.07.02|   イッテQ お祭り男宮川×手越 巨大...|神谷えりな Kamiya Erina 2|          1|2018-02-06T04:01:...|                     [none]| 90929|  442|      88|          174|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|                                 null|\n",
      "+-----------+-------------+------------------------------------+-------------------------+-----------+--------------------+---------------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+-------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the number of channels, videos and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nChannels = 4651\n",
       "nVideos = 21445\n",
       "nCategories = 17\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nChannels = df.select(\"channel_title\").distinct.count\n",
    "val nVideos = df.count\n",
    "val nCategories = df.select(\"category_id\").distinct.count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the mean and standard deviation of the number of views, comments, likes and dislikes.\n",
    "\n",
    "We use the function `df.describe()` then transform the results from `double` to `integer` type, in order to remove the decimals.\n",
    "Finally we filter out the values of max, min and number of videos, which are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+-----+--------+\n",
      "|summary|  views|comment_count|likes|dislikes|\n",
      "+-------+-------+-------------+-----+--------+\n",
      "|   mean| 262033|         1196| 8059|     366|\n",
      "| stddev|1294968|        14943|83850|    2658|\n",
      "+-------+-------+-------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numDataDouble = [summary: string, views: string ... 3 more fields]\n",
       "numData = [summary: string, views: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, views: int ... 3 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute mean, stddev, max and min values of likes, dislikes, views and comment_count\n",
    "val numDataDouble = df.describe(\"views\", \"comment_count\", \"likes\", \"dislikes\")\n",
    "\n",
    "val numData = numDataDouble.withColumn(\"views\", col(\"views\").cast(IntegerType))\n",
    "                           .withColumn(\"likes\", col(\"likes\").cast(IntegerType))\n",
    "                           .withColumn(\"dislikes\", col(\"dislikes\").cast(IntegerType))\n",
    "                           .withColumn(\"comment_count\", col(\"comment_count\").cast(IntegerType))\n",
    "                           .filter($\"summary\" =!= \"max\")\n",
    "                           .filter($\"summary\" =!= \"min\")\n",
    "                           .filter($\"summary\" =!= \"count\")\n",
    "//val numDataLong = numData.select(numData.columns.map(c => col(c).cast(IntegerType)) : _*)\n",
    "numData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular channels by grouping the dataset according to channels, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|       channel_title|num_videos|\n",
      "+--------------------+----------+\n",
      "|                null|       921|\n",
      "|           DHCテレビ|       169|\n",
      "|  釣りよかでしょう。|       145|\n",
      "|          釣りよか飯|       108|\n",
      "|           MEGWIN TV|        90|\n",
      "|    はいじぃ迷作劇場|        88|\n",
      "|          MAX鈴木 TV|        87|\n",
      "|今日ヤバイ奴に会った|        84|\n",
      "|        水溜りボンド|        82|\n",
      "|Momo and Tenももと天|        76|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByChannel = RelationalGroupedDataset: [grouping expressions: [channel_title: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "channelsVideoCount = [channel_title: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[channel_title: string, num_videos: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByChannel = df.groupBy(\"channel_title\")\n",
    "val channelsVideoCount = groupedByChannel.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "channelsVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular categories by grouping the dataset according to categories, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|category_id|num_videos|\n",
      "+-----------+----------+\n",
      "|         24|      6259|\n",
      "|         22|      3915|\n",
      "|         17|      2037|\n",
      "|         25|      1392|\n",
      "|         10|      1290|\n",
      "|          1|      1220|\n",
      "|         15|      1127|\n",
      "|         20|      1030|\n",
      "|       null|       921|\n",
      "|         26|       799|\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByCategory = RelationalGroupedDataset: [grouping expressions: [category_id: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "categoriesVideoCount = [category_id: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[category_id: string, num_videos: bigint]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByCategory = df.groupBy(\"category_id\")\n",
    "val categoriesVideoCount = groupedByCategory.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "categoriesVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data contained in the `.json` file, with the category ids, matched with the corresponding category names.\n",
    "\n",
    "The `.json` file has a complex structure, which makes it necessary to perform many steps to polish the data and obtain what we need as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|            name|\n",
      "+---+----------------+\n",
      "|  1|Film & Animation|\n",
      "|  2|Autos & Vehicles|\n",
      "| 10|           Music|\n",
      "| 15|  Pets & Animals|\n",
      "| 17|          Sports|\n",
      "+---+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoriesStruct = [etag: string, items: array<struct<etag:string,id:string,kind:string,snippet:struct<assignable:boolean,channelId:string,title:string>>> ... 1 more field]\n",
       "categoriesArray = Array(WrappedArray([\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(WrappedArray([\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load categories names file data\n",
    "val categoriesStruct = spark.read.option(\"multiline\",\"true\")\n",
    "                .json(\"data/\" + country + \"_category_id.json\")\n",
    "// extract the array inside the struct and convert to string\n",
    "val categoriesArray = categoriesStruct.collect()(0)(1)\n",
    "                .toString.split(\",\")\n",
    "// extract categories names and ids\n",
    "val names = categoriesArray.filter(x => x.contains(\"]]\")).map(x => x.dropRight(2))\n",
    "val ids = categoriesArray.filter(x => x.length() < 3)\n",
    "// create a DataFrame out of category ids and names\n",
    "val categoriesNames = sc.parallelize(ids zip names).toDF(\"id\", \"name\")\n",
    "\n",
    "categoriesNames.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join the two DataFrames of top categories and categories names, obtaining a DataFrame containing only categories names and number of videos belonging to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                name|num_videos|\n",
      "+--------------------+----------+\n",
      "|       Entertainment|      6259|\n",
      "|      People & Blogs|      3915|\n",
      "|              Sports|      2037|\n",
      "|     News & Politics|      1392|\n",
      "|               Music|      1290|\n",
      "|    Film & Animation|      1220|\n",
      "|      Pets & Animals|      1127|\n",
      "|              Gaming|      1030|\n",
      "|       Howto & Style|       799|\n",
      "|              Comedy|       743|\n",
      "|    Autos & Vehicles|       280|\n",
      "|Science & Technology|       158|\n",
      "|     Travel & Events|       143|\n",
      "|           Education|       112|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topCategories = [name: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, num_videos: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// join the dataframes so the categories are matched with their names\n",
    "val topCategories = categoriesVideoCount.join(categoriesNames, categoriesVideoCount(\"category_id\") === categoriesNames(\"id\"), \"leftouter\")\n",
    "                    .orderBy(desc(\"num_videos\"))\n",
    "                    .select(\"name\", \"num_videos\")\n",
    "                    .filter($\"name\" =!= \"null\")\n",
    "\n",
    "// hide the numeric column in the output.\n",
    "topCategories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed all the data we need, we can create a single-line DataFrame containing the data for this country.\n",
    "\n",
    "We need to transform DataFrames into Lists, which is a structure that enable us to extract single values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+--------------+----------+---------------+----------+----------------+--------------+----------+-------------+----------+----------------+\n",
      "|country|num_channels|num_videos|num_categories|views_mean|views_stddev|comments_mean|comments_stddev|likes_mean|likes_stddev|dislikes_mean|dislikes_stddev|    1_category|2_category|     3_category|4_category|      5_category|    6_category|7_category|   8_category|9_category|     10_category|\n",
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+--------------+----------+---------------+----------+----------------+--------------+----------+-------------+----------+----------------+\n",
      "|     JP|        4651|     21445|            17|    262033|     1294968|         1196|          14943|      8059|       83850|          366|           2658|People & Blogs|    Sports|News & Politics|     Music|Film & Animation|Pets & Animals|    Gaming|Howto & Style|    Comedy|Autos & Vehicles|\n",
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+--------------+----------+---------------+----------+----------------+--------------+----------+-------------+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numDataList = List([mean,262033,1196,8059,366], [stddev,1294968,14943,83850,2658])\n",
       "topCategoriesList = List([Entertainment,6259], [People & Blogs,3915], [Sports,2037], [News & Politics,1392], [Music,1290], [Film & Animation,1220], [Pets & Animals,1127], [Gaming,1030], [Howto & Style,799], [Comedy,743], [Autos & Vehicles,280], [Science & Technology,158], [Travel & Events,143], [Education,112])\n",
       "statsSeq = List((JP,4651,21445,17,262033,1294968,1196,14943,8059,83850,366,2658,People & Blogs,Sports,News & Politics,Music,Film & Animation,Pets & Animal...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((JP,4651,21445,17,262033,1294968,1196,14943,8059,83850,366,2658,People & Blogs,Sports,News & Politics,Music,Film & Animation,Pets & Animal..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numDataList = numData.rdd.collect().toList\n",
    "val topCategoriesList = topCategories.rdd.collect().toList\n",
    "\n",
    "val statsSeq = Seq((country, nChannels.toString, \n",
    "        nVideos.toString, nCategories.toString,\n",
    "        numDataList(0)(1).toString, numDataList(1)(1).toString,\n",
    "        numDataList(0)(2).toString, numDataList(1)(2).toString,\n",
    "        numDataList(0)(3).toString, numDataList(1)(3).toString,\n",
    "        numDataList(0)(4).toString, numDataList(1)(4).toString,\n",
    "        topCategoriesList(1)(0).toString, topCategoriesList(2)(0).toString,\n",
    "        topCategoriesList(3)(0).toString, topCategoriesList(4)(0).toString,\n",
    "        topCategoriesList(5)(0).toString, topCategoriesList(6)(0).toString,\n",
    "        topCategoriesList(7)(0).toString, topCategoriesList(8)(0).toString,\n",
    "        topCategoriesList(9)(0).toString, topCategoriesList(10)(0).toString))\n",
    "\n",
    "val statsRDD = spark.sparkContext.parallelize(statsSeq)\n",
    "val statsDF = statsRDD.toDF(\"country\", \"num_channels\", \n",
    "                            \"num_videos\", \"num_categories\",\n",
    "                            \"views_mean\", \"views_stddev\",\n",
    "                            \"comments_mean\", \"comments_stddev\",\n",
    "                            \"likes_mean\", \"likes_stddev\",\n",
    "                            \"dislikes_mean\", \"dislikes_stddev\",\n",
    "                            \"1_category\", \"2_category\", \n",
    "                            \"3_category\", \"4_category\",\n",
    "                            \"5_category\", \"6_category\", \n",
    "                            \"7_category\", \"8_category\",\n",
    "                            \"9_category\", \"10_category\")\n",
    "statsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allCountriesData = [country: string, num_channels: string ... 20 more fields]\n",
       "newAllCountriesData = [country: string, num_channels: string ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[country: string, num_channels: string ... 20 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val allCountriesData = spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"results/partial_\" + numCountries.toString)\n",
    "val newAllCountriesData = allCountriesData.union(statsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the final DataFrame to file, in the `results` folder.\n",
    "\n",
    "We use the function `repartition(1)` in order to generate only one file, instead of separating the data into several different files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "newAllCountriesData.repartition(1)\n",
    "             .write\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\")\n",
    "             .save(\"results/partial_\" + (numCountries + 1).toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
