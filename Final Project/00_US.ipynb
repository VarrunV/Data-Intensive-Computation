{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Youtube Trending Videos Analysis\n",
    "This notebook will load the US Youtube Trending Videos Dataset from `data` folder, and then compute and save to file in the `results` folder the following variables:\n",
    "- Number of videos, channels and categories\n",
    "- Mean and standard deviation of the number of views, comments, likes and dislikes\n",
    "- Most popular categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import useful libraries and functions, then we load data from `data` folder in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country = US\n",
       "numCountries = 0\n",
       "df = [video_id: string, trending_date: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[video_id: string, trending_date: string ... 14 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.matching\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val country = \"US\"\n",
    "val numCountries = 0\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\")\n",
    "                .load(\"data/\" + country + \"videos_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first two lines of the DataFrame (including header and the first data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---------------+-----------+--------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|  channel_title|category_id|        publish_time|                tags|  views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+---------------+-----------+--------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|2kyS6SvSYSE|     17.14.11|WE WANT TO TALK A...|   CaseyNeistat|         22|2017-11-13T17:13:...|     SHANtell martin| 748374|57527|    2966|        15954|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|SHANTELL'S CHANNE...|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|LastWeekTonight|         24|2017-11-13T07:30:...|\"last week tonigh...|2418783|97185|    6146|        12703|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|One year after th...|\n",
      "+-----------+-------------+--------------------+---------------+-----------+--------------------+--------------------+-------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the number of channels, videos and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nChannels = 2208\n",
       "nVideos = 48137\n",
       "nCategories = 17\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nChannels = df.select(\"channel_title\").distinct.count\n",
    "val nVideos = df.count\n",
    "val nCategories = df.select(\"category_id\").distinct.count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the mean and standard deviation of the number of views, comments, likes and dislikes.\n",
    "\n",
    "We use the function `df.describe()` then transform the results from `double` to `integer` type, in order to remove the decimals.\n",
    "Finally we filter out the values of max, min and number of videos, which are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------+------+--------+\n",
      "|summary|  views|comment_count| likes|dislikes|\n",
      "+-------+-------+-------------+------+--------+\n",
      "|   mean|2360784|         8446| 74266|    3711|\n",
      "| stddev|7394113|        37430|228885|   29029|\n",
      "+-------+-------+-------------+------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numDataDouble = [summary: string, views: string ... 3 more fields]\n",
       "numData = [summary: string, views: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, views: int ... 3 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute mean, stddev, max and min values of likes, dislikes, views and comment_count\n",
    "val numDataDouble = df.describe(\"views\", \"comment_count\", \"likes\", \"dislikes\")\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "val numData = numDataDouble.withColumn(\"views\", col(\"views\").cast(IntegerType))\n",
    "                           .withColumn(\"likes\", col(\"likes\").cast(IntegerType))\n",
    "                           .withColumn(\"dislikes\", col(\"dislikes\").cast(IntegerType))\n",
    "                           .withColumn(\"comment_count\", col(\"comment_count\").cast(IntegerType))\n",
    "                           .filter($\"summary\" =!= \"max\")\n",
    "                           .filter($\"summary\" =!= \"min\")\n",
    "                           .filter($\"summary\" =!= \"count\")\n",
    "//val numDataLong = numData.select(numData.columns.map(c => col(c).cast(IntegerType)) : _*)\n",
    "numData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular channels by grouping the dataset according to channels, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|       channel_title|num_videos|\n",
      "+--------------------+----------+\n",
      "|                null|      7188|\n",
      "|                ESPN|       203|\n",
      "|The Tonight Show ...|       197|\n",
      "|        TheEllenShow|       193|\n",
      "|             Netflix|       193|\n",
      "|                 Vox|       193|\n",
      "|The Late Show wit...|       187|\n",
      "|   Jimmy Kimmel Live|       186|\n",
      "|Late Night with S...|       183|\n",
      "|      Screen Junkies|       182|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByChannel = RelationalGroupedDataset: [grouping expressions: [channel_title: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "channelsVideoCount = [channel_title: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[channel_title: string, num_videos: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByChannel = df.groupBy(\"channel_title\")\n",
    "val channelsVideoCount = groupedByChannel.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "channelsVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular categories by grouping the dataset according to categories, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|category_id|num_videos|\n",
      "+-----------+----------+\n",
      "|         24|      9964|\n",
      "|       null|      7188|\n",
      "|         10|      6472|\n",
      "|         26|      4146|\n",
      "|         23|      3457|\n",
      "|         22|      3210|\n",
      "|         25|      2487|\n",
      "|         28|      2401|\n",
      "|          1|      2345|\n",
      "|         17|      2174|\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByCategory = RelationalGroupedDataset: [grouping expressions: [category_id: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "categoriesVideoCount = [category_id: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[category_id: string, num_videos: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByCategory = df.groupBy(\"category_id\")\n",
    "val categoriesVideoCount = groupedByCategory.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "categoriesVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data contained in the `.json` file, with the category ids, matched with the corresponding category names.\n",
    "\n",
    "The `.json` file has a complex structure, which makes it necessary to perform many steps to polish the data and obtain what we need as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|            name|\n",
      "+---+----------------+\n",
      "|  1|Film & Animation|\n",
      "|  2|Autos & Vehicles|\n",
      "| 10|           Music|\n",
      "| 15|  Pets & Animals|\n",
      "| 17|          Sports|\n",
      "+---+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoriesStruct = [etag: string, items: array<struct<etag:string,id:string,kind:string,snippet:struct<assignable:boolean,channelId:string,title:string>>> ... 1 more field]\n",
       "categoriesArray = Array(WrappedArray([\"m2yskBQFythfE4irbTIeOgYYfBU/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"m2yskBQFythfE4irbTIeOgYYfBU/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"m2yskBQFythfE4irbTIeOgYYfBU/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"m2yskBQFythfE4irbTIeOgYYfBU/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(WrappedArray([\"m2yskBQFythfE4irbTIeOgYYfBU/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"m2yskBQFythfE4irbTIeOgYYfBU/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"m2yskBQFythfE4irbTIeOgYYfBU/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"m2yskBQFythfE4irbTIeOgYYfBU/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load categories names file data\n",
    "val categoriesStruct = spark.read.option(\"multiline\",\"true\")\n",
    "                .json(\"data/\" + country + \"_category_id.json\")\n",
    "// extract the array inside the struct and convert to string\n",
    "val categoriesArray = categoriesStruct.collect()(0)(1)\n",
    "                .toString.split(\",\")\n",
    "// extract categories names and ids\n",
    "val names = categoriesArray.filter(x => x.contains(\"]]\")).map(x => x.dropRight(2))\n",
    "val ids = categoriesArray.filter(x => x.length() < 3)\n",
    "// create a DataFrame out of category ids and names\n",
    "val categoriesNames = sc.parallelize(ids zip names).toDF(\"id\", \"name\")\n",
    "\n",
    "categoriesNames.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join the two DataFrames of top categories and categories names, obtaining a DataFrame containing only categories names and number of videos belonging to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                name|num_videos|\n",
      "+--------------------+----------+\n",
      "|       Entertainment|      9964|\n",
      "|               Music|      6472|\n",
      "|       Howto & Style|      4146|\n",
      "|              Comedy|      3457|\n",
      "|      People & Blogs|      3210|\n",
      "|     News & Politics|      2487|\n",
      "|Science & Technology|      2401|\n",
      "|    Film & Animation|      2345|\n",
      "|              Sports|      2174|\n",
      "|           Education|      1656|\n",
      "|      Pets & Animals|       920|\n",
      "|              Gaming|       817|\n",
      "|     Travel & Events|       402|\n",
      "|    Autos & Vehicles|       384|\n",
      "|Nonprofits & Acti...|        57|\n",
      "|               Shows|        57|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topCategories = [name: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, num_videos: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// join the dataframes so the categories are matched with their names\n",
    "val topCategories = categoriesVideoCount.join(categoriesNames, categoriesVideoCount(\"category_id\") === categoriesNames(\"id\"), \"leftouter\")\n",
    "                    .orderBy(desc(\"num_videos\"))\n",
    "                    .select(\"name\", \"num_videos\")\n",
    "                    .filter($\"name\" =!= \"null\")\n",
    "\n",
    "// hide the numeric column in the output.\n",
    "topCategories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed all the data we need, we can create a single-line DataFrame containing the data for this country.\n",
    "\n",
    "We need to transform DataFrames into Lists, which is a structure that enable us to extract single values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+----------+-------------+----------+--------------+---------------+--------------------+----------------+----------+----------+--------------+\n",
      "|country|num_channels|num_videos|num_categories|views_mean|views_stddev|comments_mean|comments_stddev|likes_mean|likes_stddev|dislikes_mean|dislikes_stddev|1_category|   2_category|3_category|    4_category|     5_category|          6_category|      7_category|8_category|9_category|   10_category|\n",
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+----------+-------------+----------+--------------+---------------+--------------------+----------------+----------+----------+--------------+\n",
      "|     US|        2208|     48137|            17|   2360784|     7394113|         8446|          37430|     74266|      228885|         3711|          29029|     Music|Howto & Style|    Comedy|People & Blogs|News & Politics|Science & Technology|Film & Animation|    Sports| Education|Pets & Animals|\n",
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+----------+-------------+----------+--------------+---------------+--------------------+----------------+----------+----------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numDataList = List([mean,2360784,8446,74266,3711], [stddev,7394113,37430,228885,29029])\n",
       "topCategoriesList = List([Entertainment,9964], [Music,6472], [Howto & Style,4146], [Comedy,3457], [People & Blogs,3210], [News & Politics,2487], [Science & Technology,2401], [Film & Animation,2345], [Sports,2174], [Education,1656], [Pets & Animals,920], [Gaming,817], [Travel & Events,402], [Autos & Vehicles,384], [Nonprofits & Activism,57], [Shows,57])\n",
       "statsSeq = List((US,2208,48137,17,2360784,7394113,8446,37430,74266,228885,3711,29029,Music,Howto & Style,Co...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((US,2208,48137,17,2360784,7394113,8446,37430,74266,228885,3711,29029,Music,Howto & Style,Co..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numDataList = numData.rdd.collect().toList\n",
    "val topCategoriesList = topCategories.rdd.collect().toList\n",
    "\n",
    "val statsSeq = Seq((country, nChannels.toString, \n",
    "        nVideos.toString, nCategories.toString,\n",
    "        numDataList(0)(1).toString, numDataList(1)(1).toString,\n",
    "        numDataList(0)(2).toString, numDataList(1)(2).toString,\n",
    "        numDataList(0)(3).toString, numDataList(1)(3).toString,\n",
    "        numDataList(0)(4).toString, numDataList(1)(4).toString,\n",
    "        topCategoriesList(1)(0).toString, topCategoriesList(2)(0).toString,\n",
    "        topCategoriesList(3)(0).toString, topCategoriesList(4)(0).toString,\n",
    "        topCategoriesList(5)(0).toString, topCategoriesList(6)(0).toString,\n",
    "        topCategoriesList(7)(0).toString, topCategoriesList(8)(0).toString,\n",
    "        topCategoriesList(9)(0).toString, topCategoriesList(10)(0).toString))\n",
    "\n",
    "val statsRDD = spark.sparkContext.parallelize(statsSeq)\n",
    "val statsDF = statsRDD.toDF(\"country\", \"num_channels\", \n",
    "                            \"num_videos\", \"num_categories\",\n",
    "                            \"views_mean\", \"views_stddev\",\n",
    "                            \"comments_mean\", \"comments_stddev\",\n",
    "                            \"likes_mean\", \"likes_stddev\",\n",
    "                            \"dislikes_mean\", \"dislikes_stddev\",\n",
    "                            \"1_category\", \"2_category\", \n",
    "                            \"3_category\", \"4_category\",\n",
    "                            \"5_category\", \"6_category\", \n",
    "                            \"7_category\", \"8_category\",\n",
    "                            \"9_category\", \"10_category\")\n",
    "statsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the final DataFrame to file, in the `results` folder.\n",
    "\n",
    "We use the function `repartition(1)` in order to generate only one file, instead of separating the data into several different files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDF.repartition(1)\n",
    "             .write\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\")\n",
    "             .save(\"results/partial_\" + (numCountries + 1).toString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
