{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RU Youtube Trending Videos Analysis\n",
    "This notebook will load the Russian Youtube Trending Videos Dataset from `data` folder, and then compute and save to file in the `results` folder the following variables:\n",
    "- Number of videos, channels and categories\n",
    "- Mean and standard deviation of the number of views, comments, likes and dislikes\n",
    "- Most popular categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import useful libraries and functions, then we load data from `data` folder in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country = RU\n",
       "numCountries = 1\n",
       "df = [video_id: string, trending_date: string ... 14 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[video_id: string, trending_date: string ... 14 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.util.matching\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val country = \"RU\"\n",
    "val numCountries = 1\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/\" + country + \"videos_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first two lines of the DataFrame (including header and the first data point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|   video_id|trending_date|               title|channel_title|category_id|        publish_time|                tags| views|likes|dislikes|comment_count|      thumbnail_link|comments_disabled|ratings_disabled|video_error_or_removed|         description|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "|gDuslQ9avLc|     17.14.11|Захар и Полина уч...|    Т—Ж БОГАЧ|         22|2017-11-13T09:09:...|\"захар и полина|\"...| 62408|  334|     190|           50|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|Знакомьтесь. это ...|\n",
      "|AOCJIFEA_jE|     17.14.11|Биржа Мемов #29. ...| Druzhko Show|         22|2017-11-13T17:32:...|\"биржа мемов|\"\"ле...|330043|43841|    2244|         2977|https://i.ytimg.c...|            FALSE|           FALSE|                 FALSE|В 29 выпуске Друж...|\n",
      "+-----------+-------------+--------------------+-------------+-----------+--------------------+--------------------+------+-----+--------+-------------+--------------------+-----------------+----------------+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the number of channels, videos and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nChannels = 6932\n",
       "nVideos = 46260\n",
       "nCategories = 18\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nChannels = df.select(\"channel_title\").distinct.count\n",
    "val nVideos = df.count\n",
    "val nCategories = df.select(\"category_id\").distinct.count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the mean and standard deviation of the number of views, comments, likes and dislikes.\n",
    "\n",
    "We use the function `df.describe()` then transform the results from `double` to `integer` type, in order to remove the decimals.\n",
    "Finally we filter out the values of max, min and number of videos, which are not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+-----+--------+\n",
      "|summary| views|comment_count|likes|dislikes|\n",
      "+-------+------+-------------+-----+--------+\n",
      "|   mean|240715|         1775|12435|    1475|\n",
      "| stddev|934511|        11275|60382|    8582|\n",
      "+-------+------+-------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numDataDouble = [summary: string, views: string ... 3 more fields]\n",
       "numData = [summary: string, views: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[summary: string, views: int ... 3 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute mean, stddev, max and min values of likes, dislikes, views and comment_count\n",
    "val numDataDouble = df.describe(\"views\", \"comment_count\", \"likes\", \"dislikes\")\n",
    "\n",
    "val numData = numDataDouble.withColumn(\"views\", col(\"views\").cast(IntegerType))\n",
    "                           .withColumn(\"likes\", col(\"likes\").cast(IntegerType))\n",
    "                           .withColumn(\"dislikes\", col(\"dislikes\").cast(IntegerType))\n",
    "                           .withColumn(\"comment_count\", col(\"comment_count\").cast(IntegerType))\n",
    "                           .filter($\"summary\" =!= \"max\")\n",
    "                           .filter($\"summary\" =!= \"min\")\n",
    "                           .filter($\"summary\" =!= \"count\")\n",
    "//val numDataLong = numData.select(numData.columns.map(c => col(c).cast(IntegerType)) : _*)\n",
    "numData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular channels by grouping the dataset according to channels, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|       channel_title|num_videos|\n",
      "+--------------------+----------+\n",
      "|                null|      5521|\n",
      "|      Анатолий Шарий|       197|\n",
      "|          Эхо Москвы|       184|\n",
      "|Модные Практики с...|       169|\n",
      "|            Wylsacom|       167|\n",
      "|       kamikadzedead|       159|\n",
      "|           Россия 24|       159|\n",
      "|     Калнина Наталья|       155|\n",
      "|  След - новый сезон|       155|\n",
      "|      PanArmenian TV|       152|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByChannel = RelationalGroupedDataset: [grouping expressions: [channel_title: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "channelsVideoCount = [channel_title: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[channel_title: string, num_videos: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByChannel = df.groupBy(\"channel_title\")\n",
    "val channelsVideoCount = groupedByChannel.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "channelsVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the most popular categories by grouping the dataset according to categories, and then ordering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|category_id|num_videos|\n",
      "+-----------+----------+\n",
      "|         22|     10350|\n",
      "|         24|      5943|\n",
      "|       null|      5521|\n",
      "|         25|      5402|\n",
      "|         23|      3065|\n",
      "|          1|      3041|\n",
      "|         26|      2000|\n",
      "|         17|      1968|\n",
      "|         10|      1895|\n",
      "|          2|      1583|\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "groupedByCategory = RelationalGroupedDataset: [grouping expressions: [category_id: string], value: [video_id: string, trending_date: string ... 14 more fields], type: GroupBy]\n",
       "categoriesVideoCount = [category_id: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[category_id: string, num_videos: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val groupedByCategory = df.groupBy(\"category_id\")\n",
    "val categoriesVideoCount = groupedByCategory.count()\n",
    "                .withColumnRenamed(\"count\", \"num_videos\")\n",
    "                .orderBy(desc(\"num_videos\"))\n",
    "categoriesVideoCount.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data contained in the `.json` file, with the category ids, matched with the corresponding category names.\n",
    "\n",
    "The `.json` file has a complex structure, which makes it necessary to perform many steps to polish the data and obtain what we need as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|            name|\n",
      "+---+----------------+\n",
      "|  1|Film & Animation|\n",
      "|  2|Autos & Vehicles|\n",
      "| 10|           Music|\n",
      "| 15|  Pets & Animals|\n",
      "| 17|          Sports|\n",
      "+---+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoriesStruct = [etag: string, items: array<struct<etag:string,id:string,kind:string,snippet:struct<assignable:boolean,channelId:string,title:string>>> ... 1 more field]\n",
       "categoriesArray = Array(WrappedArray([\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(WrappedArray([\"XI7nbFXulYBIpL0ayR_gDh3eu1k/Xy1mB4_yLrHy_BmKmPBggty2mZQ\", 1, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Film & Animation]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\"\", 2, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Autos & Vehicles]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/nqRIq97-xe5XRZTxbknKFVe5Lmg\"\", 10, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Music]], \" [\"XI7nbFXulYBIpL0ayR_gDh3eu1k/HwXKamM1Q20q9BN-oBJavSGkfDI\"\", 15, youtube#videoCategory, [true, UCBR8-60-B28hp2BmDPdntcQ, Pets..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load categories names file data\n",
    "val categoriesStruct = spark.read.option(\"multiline\",\"true\")\n",
    "                .json(\"data/\" + country + \"_category_id.json\")\n",
    "// extract the array inside the struct and convert to string\n",
    "val categoriesArray = categoriesStruct.collect()(0)(1)\n",
    "                .toString.split(\",\")\n",
    "// extract categories names and ids\n",
    "val names = categoriesArray.filter(x => x.contains(\"]]\")).map(x => x.dropRight(2))\n",
    "val ids = categoriesArray.filter(x => x.length() < 3)\n",
    "// create a DataFrame out of category ids and names\n",
    "val categoriesNames = sc.parallelize(ids zip names).toDF(\"id\", \"name\")\n",
    "\n",
    "categoriesNames.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join the two DataFrames of top categories and categories names, obtaining a DataFrame containing only categories names and number of videos belonging to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                name|num_videos|\n",
      "+--------------------+----------+\n",
      "|      People & Blogs|     10350|\n",
      "|       Entertainment|      5943|\n",
      "|     News & Politics|      5402|\n",
      "|              Comedy|      3065|\n",
      "|    Film & Animation|      3041|\n",
      "|       Howto & Style|      2000|\n",
      "|              Sports|      1968|\n",
      "|               Music|      1895|\n",
      "|    Autos & Vehicles|      1583|\n",
      "|Science & Technology|      1133|\n",
      "|              Gaming|      1043|\n",
      "|           Education|       714|\n",
      "|      Pets & Animals|       604|\n",
      "|     Travel & Events|       262|\n",
      "|               Shows|       194|\n",
      "|              Movies|         1|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topCategories = [name: string, num_videos: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[name: string, num_videos: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// join the dataframes so the categories are matched with their names\n",
    "val topCategories = categoriesVideoCount.join(categoriesNames, categoriesVideoCount(\"category_id\") === categoriesNames(\"id\"), \"leftouter\")\n",
    "                    .orderBy(desc(\"num_videos\"))\n",
    "                    .select(\"name\", \"num_videos\")\n",
    "                    .filter($\"name\" =!= \"null\")\n",
    "\n",
    "// hide the numeric column in the output.\n",
    "topCategories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed all the data we need, we can create a single-line DataFrame containing the data for this country.\n",
    "\n",
    "We need to transform DataFrames into Lists, which is a structure that enable us to extract single values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+-------------+---------------+----------+----------------+-------------+----------+----------+----------------+--------------------+-----------+\n",
      "|country|num_channels|num_videos|num_categories|views_mean|views_stddev|comments_mean|comments_stddev|likes_mean|likes_stddev|dislikes_mean|dislikes_stddev|   1_category|     2_category|3_category|      4_category|   5_category|6_category|7_category|      8_category|          9_category|10_category|\n",
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+-------------+---------------+----------+----------------+-------------+----------+----------+----------------+--------------------+-----------+\n",
      "|     RU|        6932|     46260|            18|    240715|      934511|         1775|          11275|     12435|       60382|         1475|           8582|Entertainment|News & Politics|    Comedy|Film & Animation|Howto & Style|    Sports|     Music|Autos & Vehicles|Science & Technology|     Gaming|\n",
      "+-------+------------+----------+--------------+----------+------------+-------------+---------------+----------+------------+-------------+---------------+-------------+---------------+----------+----------------+-------------+----------+----------+----------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numDataList = List([mean,240715,1775,12435,1475], [stddev,934511,11275,60382,8582])\n",
       "topCategoriesList = List([People & Blogs,10350], [Entertainment,5943], [News & Politics,5402], [Comedy,3065], [Film & Animation,3041], [Howto & Style,2000], [Sports,1968], [Music,1895], [Autos & Vehicles,1583], [Science & Technology,1133], [Gaming,1043], [Education,714], [Pets & Animals,604], [Travel & Events,262], [Shows,194], [Movies,1])\n",
       "statsSeq = List((RU,6932,46260,18,240715,934511,1775,11275,12435,60382,1475,8582,Entertainment,News & Politics,Comedy,Film &...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List((RU,6932,46260,18,240715,934511,1775,11275,12435,60382,1475,8582,Entertainment,News & Politics,Comedy,Film &..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numDataList = numData.rdd.collect().toList\n",
    "val topCategoriesList = topCategories.rdd.collect().toList\n",
    "\n",
    "val statsSeq = Seq((country, nChannels.toString, \n",
    "        nVideos.toString, nCategories.toString,\n",
    "        numDataList(0)(1).toString, numDataList(1)(1).toString,\n",
    "        numDataList(0)(2).toString, numDataList(1)(2).toString,\n",
    "        numDataList(0)(3).toString, numDataList(1)(3).toString,\n",
    "        numDataList(0)(4).toString, numDataList(1)(4).toString,\n",
    "        topCategoriesList(1)(0).toString, topCategoriesList(2)(0).toString,\n",
    "        topCategoriesList(3)(0).toString, topCategoriesList(4)(0).toString,\n",
    "        topCategoriesList(5)(0).toString, topCategoriesList(6)(0).toString,\n",
    "        topCategoriesList(7)(0).toString, topCategoriesList(8)(0).toString,\n",
    "        topCategoriesList(9)(0).toString, topCategoriesList(10)(0).toString))\n",
    "\n",
    "val statsRDD = spark.sparkContext.parallelize(statsSeq)\n",
    "val statsDF = statsRDD.toDF(\"country\", \"num_channels\", \n",
    "                            \"num_videos\", \"num_categories\",\n",
    "                            \"views_mean\", \"views_stddev\",\n",
    "                            \"comments_mean\", \"comments_stddev\",\n",
    "                            \"likes_mean\", \"likes_stddev\",\n",
    "                            \"dislikes_mean\", \"dislikes_stddev\",\n",
    "                            \"1_category\", \"2_category\", \n",
    "                            \"3_category\", \"4_category\",\n",
    "                            \"5_category\", \"6_category\", \n",
    "                            \"7_category\", \"8_category\",\n",
    "                            \"9_category\", \"10_category\")\n",
    "statsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data computed previously about other countries, and join it with the data from the current country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allCountriesData = [country: string, num_channels: string ... 20 more fields]\n",
       "newAllCountriesData = [country: string, num_channels: string ... 20 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[country: string, num_channels: string ... 20 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val allCountriesData = spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .load(\"results/partial_\" + numCountries.toString)\n",
    "val newAllCountriesData = allCountriesData.union(statsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the final DataFrame to file, in the `results` folder.\n",
    "\n",
    "We use the function `repartition(1)` in order to generate only one file, instead of separating the data into several different files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "newAllCountriesData.repartition(1)\n",
    "             .write\n",
    "             .format(\"csv\")\n",
    "             .option(\"header\", \"true\")\n",
    "             .save(\"results/partial_\" + (numCountries + 1).toString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
